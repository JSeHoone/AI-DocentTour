{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í—ˆê¹…í˜ì´ìŠ¤ì˜ transformerë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•¨ \n",
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "import urllib.request\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import PreTrainedTokenizerFast"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TokenizerëŠ” ëª¨ë¸ì— ì–´ë– í•œ ì…ë ¥ì„ ë„£ì–´ì£¼ê¸° ìœ„í•´ì„œ ì „ì²˜ë¦¬ë¥¼ ë‹´ë‹¹í•©ë‹ˆë‹¤. <br>\n",
    "í† í¬ë‚˜ì´ì €ëŠ” í—ˆê¹…í˜ì´ìŠ¤ì˜ PreTrainedTokenizer ì¸ GPT2Tokenizer ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. Tokenizerë“¤ì€ í¬ê²Œ 3ê°€ì§€ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.<br>\n",
    "\n",
    "- Tokenizing : ì…ë ¥ ë¬¸ìì—´ì„ token idë¡œ ë³€í™˜(encoding), token idë¥¼ ë‹¤ì‹œ ë¬¸ìì—´ë¡œ ë³€í™˜(decoding)ì˜ ê¸°ëŠ¥<br>\n",
    "- ê¸°ì¡´ì˜ êµ¬ì¡°(BPE, Sentencepiece ë“±)ì— ë…ë¦½ì ìœ¼ë¡œ ì¶”ê°€ì ì¸ tokenë“¤ì„ ì¶”ê°€í•˜ëŠ” ê¸°ëŠ¥<br>\n",
    "- Special tokenë“¤ì„ (mask, BOS, EOS ë“±) ê´€ë¦¬í•˜ëŠ” ê¸°ëŠ¥<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS = \"</s>\" # ë¬¸ì¥ì˜ ì‹œì‘ì„ ë‚˜íƒ€ë‚´ëŠ” í† í°\n",
    "EOS = \"</s>\" # ë¬¸ì¥ì˜ ëì„ ë‚˜íƒ€ë‚´ëŠ” í† í°\n",
    "PAD = \"<pad>\" # ë™ì¼í•œ Batch ë‚´ì—ì„œ ì…ë ¥ì˜ í¬ê¸°ë¥¼ ë™ì¼í•˜ê²Œ í•˜ê¸° ìœ„í•´ì„œ ì‚¬ìš©í•˜ëŠ” í† í°\n",
    "MASK = \"<unused0>\" \n",
    "\n",
    "# í—ˆê¹…í˜ì´ìŠ¤ transformers ì— ë“±ë¡ëœ ì‚¬ì „ í•™ìŠµëœ koGTP2 í† í¬ë‚˜ì´ì €ë¥¼ ê°€ì ¸ì˜¨ë‹¤.\n",
    "koGPT2_TOKENIZER = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\", bos_token=BOS, eos_token=EOS, unk_token=\"<unk>\", pad_token=PAD, mask_token=MASK,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì±—ë´‡ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” í´ë˜ìŠ¤ë¥¼ ë§Œë“ ë‹¤.\n",
    "class ChatbotDataset(Dataset):\n",
    "    def __init__(self, chats, max_len=40):  # ë°ì´í„°ì…‹ì˜ ì „ì²˜ë¦¬ë¥¼ í•´ì£¼ëŠ” ë¶€ë¶„\n",
    "        self._data = chats\n",
    "        self.max_len = max_len\n",
    "        self.q_token = Q_TKN\n",
    "        self.a_token = A_TKN\n",
    "        self.sent_token = SENT\n",
    "        self.eos = EOS\n",
    "        self.mask = MASK\n",
    "        self.tokenizer = koGPT2_TOKENIZER\n",
    "\n",
    "    def __len__(self):  # chatbotdata ì˜ ê¸¸ì´ë¥¼ ë¦¬í„´í•œë‹¤.\n",
    "        return len(self._data)\n",
    "\n",
    "    def __getitem__(self, idx):  # ë¡œë“œí•œ ì±—ë´‡ ë°ì´í„°ë¥¼ ì°¨ë¡€ì°¨ë¡€ DataLoaderë¡œ ë„˜ê²¨ì£¼ëŠ” ë©”ì„œë“œ\n",
    "        turn = self._data.iloc[idx]\n",
    "        q = turn[\"Q\"]  # ì§ˆë¬¸ì„ ê°€ì ¸ì˜¨ë‹¤.\n",
    "        q = re.sub(r\"([?.!,])\", r\" \", q)  # êµ¬ë‘£ì ë“¤ì„ ì œê±°í•œë‹¤.\n",
    "\n",
    "        a = turn[\"A\"]  # ë‹µë³€ì„ ê°€ì ¸ì˜¨ë‹¤.\n",
    "        a = re.sub(r\"([?.!,])\", r\" \", a)  # êµ¬ë‘£ì ë“¤ì„ ì œê±°í•œë‹¤.\n",
    "\n",
    "        q_toked = self.tokenizer.tokenize(self.q_token + q + self.sent_token)\n",
    "        q_len = len(q_toked)\n",
    "\n",
    "        a_toked = self.tokenizer.tokenize(self.a_token + a + self.eos)\n",
    "        a_len = len(a_toked)\n",
    "\n",
    "        #ì§ˆë¬¸ì˜ ê¸¸ì´ê°€ ìµœëŒ€ê¸¸ì´ë³´ë‹¤ í¬ë©´\n",
    "        if q_len > self.max_len:\n",
    "            a_len = self.max_len - q_len        #ë‹µë³€ì˜ ê¸¸ì´ë¥¼ ìµœëŒ€ê¸¸ì´ - ì§ˆë¬¸ê¸¸ì´\n",
    "            if a_len <= 0:       #ì§ˆë¬¸ì˜ ê¸¸ì´ê°€ ë„ˆë¬´ ê¸¸ì–´ ì§ˆë¬¸ë§Œìœ¼ë¡œ ìµœëŒ€ ê¸¸ì´ë¥¼ ì´ˆê³¼ í•œë‹¤ë©´\n",
    "                q_toked = q_toked[-(int(self.max_len / 2)) :]   #ì§ˆë¬¸ê¸¸ì´ë¥¼ ìµœëŒ€ê¸¸ì´ì˜ ë°˜ìœ¼ë¡œ \n",
    "                q_len = len(q_toked)\n",
    "                a_len = self.max_len - q_len              #ë‹µë³€ì˜ ê¸¸ì´ë¥¼ ìµœëŒ€ê¸¸ì´ - ì§ˆë¬¸ê¸¸ì´\n",
    "            a_toked = a_toked[:a_len]\n",
    "            a_len = len(a_toked)\n",
    "\n",
    "        #ì§ˆë¬¸ì˜ ê¸¸ì´ + ë‹µë³€ì˜ ê¸¸ì´ê°€ ìµœëŒ€ê¸¸ì´ë³´ë‹¤ í¬ë©´\n",
    "        if q_len + a_len > self.max_len:\n",
    "            a_len = self.max_len - q_len        #ë‹µë³€ì˜ ê¸¸ì´ë¥¼ ìµœëŒ€ê¸¸ì´ - ì§ˆë¬¸ê¸¸ì´\n",
    "            if a_len <= 0:       #ì§ˆë¬¸ì˜ ê¸¸ì´ê°€ ë„ˆë¬´ ê¸¸ì–´ ì§ˆë¬¸ë§Œìœ¼ë¡œ ìµœëŒ€ ê¸¸ì´ë¥¼ ì´ˆê³¼ í•œë‹¤ë©´\n",
    "                q_toked = q_toked[-(int(self.max_len / 2)) :]   #ì§ˆë¬¸ê¸¸ì´ë¥¼ ìµœëŒ€ê¸¸ì´ì˜ ë°˜ìœ¼ë¡œ \n",
    "                q_len = len(q_toked)\n",
    "                a_len = self.max_len - q_len              #ë‹µë³€ì˜ ê¸¸ì´ë¥¼ ìµœëŒ€ê¸¸ì´ - ì§ˆë¬¸ê¸¸ì´\n",
    "            a_toked = a_toked[:a_len]\n",
    "            a_len = len(a_toked)\n",
    "\n",
    "        # ë‹µë³€ labels = [mask, mask, ...., mask, ..., <bos>,..ë‹µë³€.. <eos>, <pad>....]\n",
    "        labels = [self.mask,] * q_len + a_toked[1:]\n",
    "\n",
    "        # mask = ì§ˆë¬¸ê¸¸ì´ 0 + ë‹µë³€ê¸¸ì´ 1 + ë‚˜ë¨¸ì§€ 0\n",
    "        mask = [0] * q_len + [1] * a_len + [0] * (self.max_len - q_len - a_len)\n",
    "        # ë‹µë³€ labelsì„ index ë¡œ ë§Œë“ ë‹¤.\n",
    "        labels_ids = self.tokenizer.convert_tokens_to_ids(labels)\n",
    "        # ìµœëŒ€ê¸¸ì´ë§Œí¼ PADDING\n",
    "        while len(labels_ids) < self.max_len:\n",
    "            labels_ids += [self.tokenizer.pad_token_id]\n",
    "\n",
    "        # ì§ˆë¬¸ + ë‹µë³€ì„ index ë¡œ ë§Œë“ ë‹¤.    \n",
    "        token_ids = self.tokenizer.convert_tokens_to_ids(q_toked + a_toked)\n",
    "        # ìµœëŒ€ê¸¸ì´ë§Œí¼ PADDING\n",
    "        while len(token_ids) < self.max_len:\n",
    "            token_ids += [self.tokenizer.pad_token_id]\n",
    "\n",
    "        #ì§ˆë¬¸+ë‹µë³€, ë§ˆìŠ¤í¬, ë‹µë³€\n",
    "        return (token_ids, np.array(mask), labels_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "KoGPT2 ëª¨ë¸ ë¶ˆëŸ¬ì™€ì„œ ì‚¬ìš©í•´ë³´ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['â–ì•ˆë…•',\n",
       " 'í•˜',\n",
       " 'ì„¸',\n",
       " 'ìš”.',\n",
       " 'â–í•œêµ­ì–´',\n",
       " 'â–G',\n",
       " 'P',\n",
       " 'T',\n",
       " '-2',\n",
       " 'â–ì…',\n",
       " 'ë‹ˆë‹¤.',\n",
       " 'ğŸ˜¤',\n",
       " ':)',\n",
       " 'l^o']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\", bos_token='</s>', eos_token='</s>', unk_token='<unk>', pad_token='<pad>', mask_token='<mask>') \n",
    "tokenizer.tokenize(\"ì•ˆë…•í•˜ì„¸ìš”. í•œêµ­ì–´ GPT-2 ì…ë‹ˆë‹¤.ğŸ˜¤:)l^o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35bfd8f034244ea299b2ee46b00fb538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/513M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê·¼ìœ¡ì´ ì»¤ì§€ê¸° ìœ„í•´ì„œëŠ” ë¬´ì—‡ë³´ë‹¤ ê·œì¹™ì ì¸ ìƒí™œìŠµê´€ì´ ì¤‘ìš”í•˜ë‹¤.\n",
      "íŠ¹íˆ, ì•„ì¹¨ì‹ì‚¬ëŠ” ë‹¨ë°±ì§ˆê³¼ ë¹„íƒ€ë¯¼ì´ í’ë¶€í•œ ê³¼ì¼ê³¼ ì±„ì†Œë¥¼ ë§ì´ ì„­ì·¨í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤.\n",
      "ë˜í•œ í•˜ë£¨ 30ë¶„ ì´ìƒ ì¶©ë¶„í•œ ìˆ˜ë©´ì„ ì·¨í•˜ëŠ” ê²ƒë„ ë„ì›€ì´ ëœë‹¤.\n",
      "ì•„ì¹¨ ì‹ì‚¬ë¥¼ ê±°ë¥´ì§€ ì•Šê³  ê·œì¹™ì ìœ¼ë¡œ ìš´ë™ì„ í•˜ë©´ í˜ˆì•¡ìˆœí™˜ì— ë„ì›€ì„ ì¤„ ë¿ë§Œ ì•„ë‹ˆë¼ ì‹ ì§„ëŒ€ì‚¬ë¥¼ ì´‰ì§„í•´ ì²´ë‚´ ë…¸íë¬¼ì„ ë°°ì¶œí•˜ê³  í˜ˆì••ì„ ë‚®ì¶°ì¤€ë‹¤.\n",
      "ìš´ë™ì€ í•˜ë£¨ì— 10ë¶„ ì •ë„ë§Œ í•˜ëŠ” ê²Œ ì¢‹ìœ¼ë©° ìš´ë™ í›„ì—ëŠ” ë°˜ë“œì‹œ ìŠ¤íŠ¸ë ˆì¹­ì„ í†µí•´ ê·¼ìœ¡ëŸ‰ì„ ëŠ˜ë¦¬ê³  ìœ ì—°ì„±ì„ ë†’ì—¬ì•¼ í•œë‹¤.\n",
      "ìš´ë™ í›„ ë°”ë¡œ ì ìë¦¬ì— ë“œëŠ” ê²ƒì€ í”¼í•´ì•¼ í•˜ë©° íŠ¹íˆ ì•„ì¹¨ì— ì¼ì–´ë‚˜ë©´ ëª¸ì´ í”¼ê³¤í•´ì§€ê¸° ë•Œë¬¸ì— ë¬´ë¦¬í•˜ê²Œ ì›€ì§ì´ë©´ ì˜¤íˆë ¤ ì—­íš¨ê³¼ê°€ ë‚  ìˆ˜ë„ ìˆë‹¤.\n",
      "ìš´ë™ì„\n"
     ]
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
    "\n",
    "text = 'ê·¼ìœ¡ì´ ì»¤ì§€ê¸° ìœ„í•´ì„œëŠ”'\n",
    "input_ids = tokenizer.encode(text)\n",
    "gen_ids = model.generate(torch.tensor([input_ids]),\n",
    "                           max_length=128,\n",
    "                           repetition_penalty=2.0,\n",
    "                           pad_token_id=tokenizer.pad_token_id,\n",
    "                           eos_token_id=tokenizer.eos_token_id,\n",
    "                           bos_token_id=tokenizer.bos_token_id,\n",
    "                           use_cache=True)\n",
    "generated = tokenizer.decode(gen_ids[0,:].tolist())\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q. ëŒ€í•œë¯¼êµ­ ì˜ˆì‚°êµ°ì˜ ì „í†µ ì‹œì¥ì— ëŒ€í•´ì„œ ì•Œë ¤ì¤˜ì•¼ê² ë‹¤.\n",
      "ì´ë²ˆì—ëŠ” ìš°ë¦¬ êµ°ì˜ ì¬ë˜ì‹œì¥ í˜„ëŒ€í™” ì‚¬ì—…ì— ëŒ€í•´ ì•Œì•„ë³´ë„ë¡ í•˜ê² ë‹¤.\n",
      "ìš°ë¦¬ êµ°ì€ ì§€ë‚œí•´ë¶€í„° ì‹œì¥ ìƒì¸íšŒì™€ í•¨ê»˜ ìƒì¸ë“¤ì˜ ì˜ê²¬ì„ ìˆ˜ë ´í•˜ì—¬ ì‚¬ì—…ì„ ì¶”ì§„í•´ì™”ë‹¤.\n",
      "ê·¸ ê²°ê³¼ ì˜¬í•´ëŠ” ì´ ì‚¬ì—…ë¹„ 4ì–µ ì›ì„ íˆ¬ì…, ê¸°ì¡´ ì‹œì¥ì„ ë¦¬ëª¨ë¸ë§í•˜ê³  ì•„ì¼€ì´ë“œì™€ ì£¼ì°¨ì¥ì„ ì¡°ì„±í•˜ëŠ” ë“± í˜„ëŒ€ì‹ ì‹œì¥ìœ¼ë¡œ íƒˆë°”ê¿ˆí–ˆë‹¤.\n",
      "ë˜í•œ ì§€ì—­ ë‚´ ìš°ìˆ˜ ë†íŠ¹ì‚°ë¬¼ì„ í™œìš©í•œ ë‹¤ì–‘í•œ ë¨¹ê±°ë¦¬ ì¥í„°ë¥¼ ìš´ì˜í•¨ìœ¼ë¡œì¨ ì†Œë¹„ìë“¤ì—ê²Œ ë³´ë‹¤ ë§ì€ ë³¼ê±°ë¦¬ë¥¼ ì œê³µí•  ìˆ˜ ìˆê²Œ ë˜ì—ˆë‹¤.\n",
      "íŠ¹íˆ ì´ë²ˆ ì‚¬ì—…ì€ êµ°ì—ì„œ ì¶”ì§„í•˜ëŠ” ì‚¬ì—…ìœ¼ë¡œ ì „êµ­ ìµœì´ˆë¡œ ì‹œí–‰í•˜ëŠ” ê²ƒìœ¼ë¡œ ê·¸ ì˜ë¯¸ê°€ í¬ë‹¤.\n",
      "ì•ìœ¼ë¡œ ì´ ì‚¬ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ë§ˆë¬´ë¦¬ë˜ë©´ ì¹¨ì²´ëœ ê²½ê¸° í™œì„±í™”ì— í° ë„ì›€ì´ ë \n"
     ]
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')\n",
    "\n",
    "text = 'Q. ëŒ€í•œë¯¼êµ­ ì˜ˆì‚°êµ°ì˜ ì „í†µ ì‹œì¥ì— ëŒ€í•´ì„œ ì•Œë ¤ì¤˜'\n",
    "input_ids = tokenizer.encode(text)\n",
    "gen_ids = model.generate(torch.tensor([input_ids]),\n",
    "                           max_length=128,\n",
    "                           repetition_penalty=2.0,\n",
    "                           pad_token_id=tokenizer.pad_token_id,\n",
    "                           eos_token_id=tokenizer.eos_token_id,\n",
    "                           bos_token_id=tokenizer.bos_token_id,\n",
    "                           use_cache=True)\n",
    "generated = tokenizer.decode(gen_ids[0,:].tolist())\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\save_desktop\\\\study_project'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ì—¬í–‰ ì¼ì • : 2 ë°• 3ì¼\n",
    "ì–´ëŠì§€ì—­ : ì˜ˆì‚°êµ°\n",
    "ëˆ„êµ¬ë‘ : ê°€ì¡±ë“¤ì´ë‘\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s ê°’ì„ ê°€ì¥ ì‘ê²Œ í•˜ê¸° ìœ„í•œ Aì˜ ìˆ˜ë¥¼ ì¬ë°°ì—´\n",
    "# Aì˜ ìˆ˜ë‘ Bì˜ ìˆ˜ë¥¼ ê³±í•˜ê³  ê·¸ê²ƒë“¤ì„ ë”í•´ì£¼ëŠ” \n",
    "# ì´ ìˆ˜ë¥¼ ê°€ì¥ ì‘ê²Œ í•˜ê¸° ìœ„í•´ì„œëŠ” ì •ë ¬ì„ í•œë‹¤\n",
    "\n",
    "import sys\n",
    "input = sys.stdin.readline\n",
    "\n",
    "cnt_num = int(input())\n",
    "A = list(map(int,input().split())).sort()\n",
    "B = list(map(int,input().split())).sort()\n",
    "\n",
    "\n",
    "answer = 0\n",
    "for a,b in zip(A,B):\n",
    "    data = a*b\n",
    "    answer += data\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,io\n",
    "file = open(\"inputs.txt\",'w')\n",
    "data='''3 4\n",
    "ohhenrie\n",
    "charlie\n",
    "baesangwook\n",
    "obama\n",
    "baesangwook\n",
    "ohhenrie\n",
    "clinton\n",
    "'''\n",
    "\n",
    "file.write(data)\n",
    "file.close()\n",
    "input_file = open('inputs.txt',\"r\")\n",
    "sys.stdin = io.StringIO(input_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "baesangwook\n",
      "ohhenrie\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ohhenrie': 0, 'charlie': 0, 'baesangwook': 0}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = sys.stdin.readline\n",
    "\n",
    "N,M = map(int,input().split())\n",
    "\n",
    "nosee = dict()\n",
    "for _ in range(N):\n",
    "    name = input().rstrip()\n",
    "    nosee[f'{name}'] = 0\n",
    "\n",
    "answer = []\n",
    "for _ in range(M):\n",
    "    name = input().rstrip()\n",
    "    if name in nosee:\n",
    "        answer.append(name)\n",
    "\n",
    "print(len(answer))\n",
    "\n",
    "for name_ in answer:\n",
    "    print(name_)\n",
    "# # N = ë“£ë„ M = ë³´ë„B\n",
    "# if N > M:\n",
    "\n",
    "nosee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['obama\\n', 'baesangwook\\n', 'ohhenrie\\n', 'clinton\\n']\n"
     ]
    }
   ],
   "source": [
    "print(nolisten)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
